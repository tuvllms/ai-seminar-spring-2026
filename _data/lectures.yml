- date: 8/26
  lecturer:
  title: <strong>Introduction</strong>
  slides: "https://tuvllms.github.io/ai-fall-2025/assets/pdf/lecture_1.pdf"
  readings:
    - <a>Russell and Norvig, Chapter 1</a>
  logistics:

- date: 8/28
  lecturer:
  title: <strong>Language Modeling</strong>
  slides: "https://tuvllms.github.io/ai-fall-2025/assets/pdf/lecture_2.pdf"
  readings:
    - <a href="https://web.stanford.edu/~jurafsky/slp3/3.pdf" target="_blank">Jurafsky and Martin, Chapter 3.1-3.5</a>
  logistics:

- date: 9/2
  lecturer:
  title: >
    <strong>Neural networks</strong>
  slides: "https://tuvllms.github.io/ai-fall-2025/assets/pdf/lecture_3.pdf"
  readings:
    - <a href="https://web.stanford.edu/~jurafsky/slp3/6.pdf" target="_blank">Jurafsky and Martin, Chapter 6.1-6.5</a>
    - Bengio et al. (2003) <a href="https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf" target="_blank">A Neural Probabilistic Language Model</a>
  logistics:

- date: 9/4
  lecturer:
  title: <strong>Backpropagation</strong>
  slides: "https://tuvllms.github.io/ai-fall-2025/assets/pdf/lecture_4.pdf"
  readings:
    - <a href="https://web.stanford.edu/~jurafsky/slp3/6.pdf" target="_blank">Jurafsky and Martin, Chapter 6.6</a>
  logistics:

- date: 9/9
  lecturer:
  title: <strong>Embeddings</strong>
  slides: "https://tuvllms.github.io/ai-fall-2025/assets/pdf/lecture_5.pdf"
  readings:
    - <a href="https://web.stanford.edu/~jurafsky/slp3/5.pdf" target="_blank">Jurafsky and Martin, Chapter 5</a>
  logistics:

- date: 9/11
  lecturer:
  title: <strong>Transformers</strong>
  slides: "https://tuvllms.github.io/ai-fall-2025/assets/pdf/lecture_6.pdf"
  readings:
     - Vaswani et al. (2017) <a href="https://arxiv.org/pdf/1706.03762" target="_blank">Attention Is All You Need</a>
  logistics:

- date: 9/16
  lecturer:
  title: <strong>Transformers (cont.)</strong>
  slides: "https://tuvllms.github.io/ai-fall-2025/assets/pdf/lecture_7.pdf"
  readings:
    - "Devlin et al. (2018) <a href='https://arxiv.org/pdf/1810.04805' target='_blank'>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a>"
    - Raffel et al. (2019) <a href="https://arxiv.org/pdf/1910.10683" target="_blank">Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</a>
    - Radford et al. (2019) <a href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf" target="_blank">Language Models are Unsupervised Multitask Learners</a>
  logistics:

- date: 9/18
  lecturer:
  title: <strong>Pretraining scaling</strong>
  slides: "https://tuvllms.github.io/ai-fall-2025/assets/pdf/lecture_8.pdf"
  readings:
    - Kaplan et al. (2020) <a href="https://arxiv.org/pdf/2001.08361" target="_blank">Scaling Laws for Neural Language Models</a>
    - Hoffmann et al. (2022) <a href="https://arxiv.org/pdf/2203.15556" target="_blank">Training Compute-Optimal Large Language Models</a>
    - "Li et al. (2025) <a href='https://arxiv.org/pdf/2502.18969' target='_blank'>(Mis)Fitting: A Survey of Scaling Laws</a>"
  logistics:

- date: 9/23
  lecturer:
  title: <strong>Multimodal models</strong>
  slides: "https://tuvllms.github.io/ai-fall-2025/assets/pdf/lecture_9.pdf"
  readings:
    - "Wang et al. (2024) <a href='https://arxiv.org/abs/2409.12191' target='_blank'>Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution</a>"
    - "McKinzie et al. (2024) <a href='https://arxiv.org/abs/2403.09611' target='_blank'>MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training</a>"
    - "Sebastian Raschka <a href='https://sebastianraschka.com/blog/2024/understanding-multimodal-llms.html' target='_blank'>Understanding Multimodal LLMs</a>"
    - "[optional] Deitke et al. (2024) <a href='https://arxiv.org/abs/2409.17146' target='_blank'>Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Vision-Language Models</a>"
    - "[optional] Wang et al. (2024) <a href='https://arxiv.org/abs/2409.18869' target='_blank'>Emu3: Next-Token Prediction is All You Need</a>"
    - "[optional] Jiang et al. (2025) <a href='https://arxiv.org/abs/2503.04130' target='_blank'>Token-Efficient Long Video Understanding for Multimodal LLMs</a>"
  logistics:

- date: 9/25
  title: No classes (Tu is OOO)

- date: 9/30
  lecturer:
  title: <strong>Prompting</strong>
  slides: "https://tuvllms.github.io/ai-fall-2025/assets/pdf/lecture_10.pdf"
  readings:
    - Brown et al. (2020) <a href="https://arxiv.org/pdf/2005.14165" target="_blank">Language Models are Few-Shot Learners</a>
    - Wei et al. (2022) <a href="https://arxiv.org/pdf/2201.11903" target="_blank">Chain-of-Thought Prompting Elicits Reasoning in Large Language Models</a>
  logistics:

- date: 10/2
  lecturer:
  title: <strong>Decoding strategies</strong>
  slides: "https://tuvllms.github.io/ai-fall-2025/assets/pdf/lecture_11.pdf"
  readings:
    - Holtzman et al. (2019) <a href="https://arxiv.org/pdf/1904.09751" target="_blank">The Curious Case of Neural Text Degeneration</a>
  logistics:

- date: 10/7
  lecturer:
  title: <strong>Instruction tuning</strong>
  slides: "https://tuvllms.github.io/ai-fall-2025/assets/pdf/lecture_12.pdf"
  readings:
    - Wei et al. (2021) <a href="https://arxiv.org/pdf/2109.01652" target="_blank">Finetuned Language Models Are Zero-Shot Learners</a>
    - Chung et al. (2022) <a href="https://arxiv.org/pdf/2210.11416" target="_blank">Scaling Instruction-Finetuned Language Models</a>
    - "[optional] Sanh et al. (2021) <a href='https://arxiv.org/pdf/2110.08207' target='_blank'>Multitask Prompted Training Enables Zero-Shot Task Generalization</a>"
    - "[optional] Longpre et al. (2023) <a href='https://arxiv.org/pdf/2301.13688' target='_blank'>The Flan Collection: Designing Data and Methods for Effective Instruction Tuning</a>"
  logistics:

- date: 10/9
  lecturer:
  title: <strong>Alignment</strong>
  slides: "https://tuvllms.github.io/ai-fall-2025/assets/pdf/lecture_13.pdf"
  readings:
    - Ouyang et al. (2022) <a href="https://arxiv.org/pdf/2203.02155" target="_blank">Training language models to follow instructions with human feedback</a>
    - Bai et al. (2022) <a href="https://arxiv.org/pdf/2204.05862" target="_blank">Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback</a>
    - "Rafailov et al. (2023) <a href='https://arxiv.org/pdf/2305.18290' target='_blank'>Direct Preference Optimization: Your Language Model is Secretly a Reward Model</a>"
  logistics:

- date: 10/14
  lecturer:
  title: <strong>Large reasoning models & Test-time scaling</strong>
  slides: "https://tuvllms.github.io/ai-fall-2025/assets/pdf/lecture_14.pdf"
  readings:
    - "DeepSeek-AI (2025) <a href='https://arxiv.org/pdf/2501.19393' target='_blank'>DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning</a>"
    - "Muennighoff et al. (2025) <a href='https://arxiv.org/pdf/2501.19393' target='_blank'>s1: Simple test-time scaling</a>"
    - "Brown et al. (2024) <a href='https://arxiv.org/pdf/2407.21787' target='_blank'>Large language monkeys: Scaling inference compute with repeated sampling</a>"
    - "[optional] Geiping et al. (2025) <a href='https://www.arxiv.org/pdf/2502.05171' target='_blank'>Scaling up Test-Time Compute with Latent Reasoning: A Recurrent Depth Approach</a>"
  logistics:

- date: 10/16
  lecturer:
  title: >
    <strong>Large reasoning models & Test-time scaling (cont'd)</strong>
  slides: "https://tuvllms.github.io/ai-fall-2025/assets/pdf/lecture_15.pdf"
  readings:
    - "Ye et al. (2025) <a href='https://arxiv.org/pdf/2502.03387' target='_blank'>LIMO: Less is More for Reasoning</a>"
    - "Yu et al. (2025) <a href='https://arxiv.org/pdf/2504.00810' target='_blank'>Z1: Efficient Test-time Scaling with Code</a>"
    - "[optional] Xiang et al. (2025) <a href='https://arxiv.org/pdf/2501.04682' target='_blank'>Towards System 2 Reasoning in LLMs: Learning How to Think With Meta Chain-of-Thought</a>"
  logistics:

- date: 10/21
  lecturer:
  title: <strong>Evaluation</strong>
  slides: "https://tuvllms.github.io/ai-fall-2025/assets/pdf/lecture_16.pdf"
  readings:
    - Zheng et al. (2023) <a href="https://arxiv.org/pdf/2306.05685" target="_blank">Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena</a>
    - "[optional] Vu et al. (2024) <a href='https://arxiv.org/pdf/2407.10817' target='_blank'>Foundational Autoraters: Taming Large Language Models for Better Automatic Evaluation</a>"
  logistics:

- date: 10/23
  lecturer: 
  title: <strong>Mixture-of-Experts</strong>
  slides: "https://tuvllms.github.io/ai-fall-2025/assets/pdf/lecture_17.pdf"
  readings:
    - "Fedus et al. (2021) <a href='https://arxiv.org/pdf/2101.03961' target='_blank'>Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity</a>"
    - "Shen et al. (2023) <a href='https://arxiv.org/pdf/2305.14705' target='_blank'>Mixture-of-Experts Meets Instruction Tuning: A Winning Combination for Large Language Models</a>"
    - "[optional] Zoph et al. (2022) <a href='https://arxiv.org/pdf/2202.08906' target='_blank'>ST-MoE: Designing Stable and Transferable Sparse Expert Models</a>"
    - "[optional] Lepikhin et al. (2020) <a href='https://arxiv.org/pdf/2006.16668' target='_blank'>GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding</a>"
  logistics:

- date: 10/28
  title: No classes (Tu is OOO)

- date: 10/30
  title: <strong>Efficient attention</strong>
  slides: "https://tuvllms.github.io/ai-fall-2025/assets/pdf/lecture_18.pdf"
  readings:
    - "Dao et al. (2022) <a href='https://arxiv.org/pdf/2205.14135' target='_blank'>FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness</a>"
    - "Gao et al. (2024) <a href='https://arxiv.org/pdf/2410.02660' target='_blank'>How to Train Long-Context Language Models (Effectively)</a>"
  logistics:
  
- date: 11/4
  lecturer:
  title: <strong>Parameter-efficient fine-tuning</strong>
  slides: "https://tuvllms.github.io/ai-fall-2025/assets/pdf/lecture_19.pdf"
  readings:
    - "Hu et al. (2021) <a href='https://arxiv.org/pdf/2106.09685' target='_blank'>LoRA: Low-Rank Adaptation of Large Language Models</a>"
    - Lester et al. (2021) <a href="https://arxiv.org/pdf/2104.08691" target="_blank">The Power of Scale for Parameter-Efficient Prompt Tuning</a>
    - "[optional] Vu et al. (2022) <a href='https://arxiv.org/pdf/2110.07904' target='_blank'>SPoT: Better Frozen Model Adaptation through Soft Prompt Transfer</a>"
  logistics:

- date: 11/6
  title: <strong>Efficient training and inference</strong>
  slides: "https://tuvllms.github.io/ai-fall-2025/assets/pdf/lecture_20.pdf"
  readings:
    - Hinton et al. (2015) <a href="https://arxiv.org/pdf/1503.02531" target="_blank">Distilling the Knowledge in a Neural Network</a>
    - Ilharco et al. (2022) <a href="https://arxiv.org/pdf/2212.04089" target="_blank">Editing Models with Task Arithmetic</a>
    - Maarten Grootendorst's blog <a href="https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-quantization" target="_blank">A Visual Guide to Quantization</a>
    
  logistics:

- date: 11/11
  lecturer:
  title: <strong>Retrieval-augmented generation & Tool-use models</strong>
  slides: "https://tuvllms.github.io/ai-fall-2025/assets/pdf/lecture_21.pdf"
  readings:
    - "Lewis et al. (2020) <a href='https://arxiv.org/pdf/2005.11401' target='_blank'>Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks</a>"
    - "Schick et al. (2023) <a href='https://arxiv.org/pdf/2302.04761' target='_blank'>Toolformer: Language Models Can Teach Themselves to Use Tools</a>"
    - "[optional] Jin et al. (2025) <a href='https://arxiv.org/pdf/2503.09516' target='_blank'>Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning</a>"

  logistics:

- date: 11/13
  lecturer:
  title: <strong>LLM Agents</strong>
  slides: "https://tuvllms.github.io/ai-fall-2025/assets/pdf/lecture_22.pdf"
  readings:
    - "Andrew Ng <a href='https://www.deeplearning.ai/the-batch/how-agents-can-improve-llm-performance/' target='_blank'>Agentic Design Patterns Part 1-5</a>"
    - "[optional] Khattab et al. (2023) <a href='https://arxiv.org/pdf/2310.03714' target='_blank'>DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines</a>"
    - "[optional] Agrawal et al. (2025) <a href='https://arxiv.org/pdf/2507.19457' target='_blank'>GEPA: Reflective Prompt Evolution Can Outperform Reinforcement Learning
</a>"
    
    
  logistics:

- date: 11/18
  lecturer:
  title: <strong>Diffusion models</strong>
  slides: "https://tuvllms.github.io/ai-fall-2025/assets/pdf/lecture_23.pdf"
  readings:
  logistics:

- date: 11/20
  lecturer: 
  title: <strong>Ethics and safety</strong>
  slides: "https://tuvllms.github.io/ai-fall-2025/assets/pdf/lecture_24.pdf"
  readings:
  logistics:

- date: 11/25
  title: No classes (Thanksgiving break)

- date: 11/27
  title: No classes (Thanksgiving break)

- date: 12/2
  title: No classes

- date: 12/4
  title: "Project presentations"

- date: 12/9
  title: "Project presentations"


